# -*- coding: utf-8 -*-
"""Projet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1259N_PEeLfx1EY1N8tJu1A-L6GU_BA5v
"""

#librairie pandas
import pandas as pd
#Stat - librairie numpy
import numpy as np
#scikit-learn
import sklearn
#scree plot 
import matplotlib.pyplot as plt
#classe StandardScaler pour standardisation (centrage et reduction)
from sklearn.preprocessing import StandardScaler
#classe pour l'ACP
from sklearn.decomposition import PCA
#kmeans
from sklearn import cluster
#librairies pour la CAH
from scipy.cluster.hierarchy import dendrogram , linkage, fcluster

#Lire le fichier Excel et afficher les premières lignes
df = pd.read_excel("baywheels-tripdata.xls")
print(df.head())

"""# **Statistiques descriptives**"""

#dimension
print(df.shape) 
n, p = df.shape
print(f"Nombre d'individus: {n}") #lignes
print(f"Nombre de variables: {p}") #colonnes

#Afficher les statistiques descriptives pour toutes les colonnes
print(df.describe(include='all'))

print(df[['duration_sec','member_birth_year']])

print(f"Moyenne de duration_sec: {df['duration_sec'].mean()}")
print(f"Moyenne de member_birth_year: {df['member_birth_year'].mean()}")

#Afficher le nombre de valeurs uniques dans la colonne member_birth_year
print(df.member_birth_year.value_counts())

#Afficher les lignes où duration_sec est supérieur ou égal à 1/4 heure
print(df.loc[df['duration_sec']>=900,:])

#Afficher les lignes où duration_sec est supérieur ou égal à 1/4 heure et les members agées 35 ans ou plus
print(df.loc[(df['duration_sec']>=900) & (df['member_birth_year'] <= 1988),:])

# Drop the "user_type" column
df_numeric = df.drop("user_type", axis=1)
#instanciation
sc = StandardScaler()
#transformation – centrage-réduction
Z = sc.fit_transform(df_numeric) 
print(Z)
#Afficher la moyenne et l’écart type de chaque colonne
print(f"Moyenne des données normalisées:\n{np.mean(Z, axis=0)}")
print(f"Écart-type des données normalisées:\n{np.std(Z, axis=0, ddof=0)}")

#Matrice de corrélation
corr = df.corr()
df.corr()

#VERIFICATION: 1/n (Z'.Z) : matrice de corrélation
T=np.transpose(Z)
print((T.dot(Z))/n)

"""# **Distribution**"""

#Tracer une estimation de densité noyau de la colonne duration_sec
df['duration_sec'].plot.kde()
plt.show()

#distributions avec un boxplot de la colonne duration_sec
df.boxplot(column='duration_sec')
plt.show()

#Nuage de points de duration_sec par rapport à member_birth_year
df.plot.scatter(x='duration_sec',y='member_birth_year')
plt.show()

#scatter_matrix
pd.plotting.scatter_matrix(df,hist_kwds={'bins':30})
plt.show()

pd.plotting.scatter_matrix(df,diagonal='kde')
plt.show()

#Grouper les données par bike_id 
grouped= df.groupby('bike_id')
print(grouped[['duration_sec']].describe())

#Créer un histogramme de la colonne member_birth_year
df.hist(column='member_birth_year')
plt.show()

#Créer une estimation de la densité du noyau de la colonne member_birth_year
df['member_birth_year'].plot.kde()
plt.show()

df.boxplot(column='duration_sec',by='user_type')
plt.show()

"""# **ACP**"""

#Effectuer l’analyse des composantes principales (APC) des données
acp = PCA()
acp.fit(Z)

#centrage & réduction
coord = acp.fit_transform(Z)
print(coord)

print(np.var(coord,axis=0))       #variance
print(np.mean(coord,axis=0))      #moyenne=0

eigvals,eigvecs = np.linalg.eig(corr)
print(f"Eigenvalues:\n{eigvals}")
print(f"Eigenvectors:\n{eigvecs}")
#axe 0 & 1 & 2 & 3 > 1

#Calculer la variance expliquée et la variance expliquée cumulée des composantes principales
tot= sum(eigvals)
var_exp = [(i/tot)*100 for i in sorted(eigvals,reverse= True)]
cum_var_exp = np.cumsum(var_exp)
print(f"Variance expliquée:\n{var_exp}")
print(f"Variance expliquée cumulée:\n{cum_var_exp}")
#proportion de l'inertie expliqué par les axes choisies:72.97961

eigval=np.sort(eigvals)
valprop=eigval[::-1]
print(pd.DataFrame({'valprop':valprop,'inertie':var_exp,'inertiecum':cum_var_exp, }))

#variance expliquée
print(acp.explained_variance_)
#proportion de variance expliquée
print(acp.explained_variance_ratio_)

#cumul de variance expliquée
plt.plot(np.arange(1,p+1), np.concatenate(([0], cum_var_exp)))
plt.title("Explained variance vs. # of factors")
plt.ylabel("Cumsum explained variance ratio")
plt.xlabel("Factor number")
plt.show()

print(acp.components_)

#contribution des individus dans l'inertie totale
A=coord**2                              #mult par element
Ctr1=(A[:,0]/(n*eigvals[0]))*100        #contribution 1er axe
B=Z**2
normind=np.sum(Z**2,axis=1)
Cos1=(A[:,0]/normind)*100

#contribution des individus dans l'inertie totale
A=coord**2
Ctr2=(A[:,1]/(n*eigvals[1]))*100          #contribution 2eme axe
Cos2=(A[:,1]/normind)*100

print(pd.DataFrame({'ID':df.index,'Contr_1':Ctr1,'Contr_2':Ctr2,'Coscar_1':Cos1,'Coscar_2':Cos2, }))

#positionnement des individus dans le premier plan
fig, axes = plt.subplots(figsize=(12,12))
axes.set_xlim(-6,6) #même limites en abscisse
axes.set_ylim(-6,6) #et en ordonnée
#placement des étiquettes des observations
for i in range(n):
 plt.annotate(df.index[i],(coord[i,0],coord[i,1]))
#ajouter les axes
plt.plot([-6,6],[0,0],color='silver',linestyle='-',linewidth=1)
plt.plot([0,0],[-6,6],color='silver',linestyle='-',linewidth=1)
#affichage
plt.show()

sqrt_eigval = np.sqrt(valprop)
corvar1=acp.components_[0,:] * sqrt_eigval[0]
print(corvar1)

corvar2=acp.components_[1,:] * sqrt_eigval[1]
print(corvar2)

#contribution des variables 
Ctrvar1=(corvar1**2)/(valprop[0])*100
Ctrvar2=(corvar2**2)/(valprop[1])*100
#qualité des variables 
Cosvar1=(corvar1**2)*100
Cosvar2=(corvar2**2)*100

print(pd.DataFrame({'ID':df.columns,'corvar1':corvar1[:-1],'corvar2':corvar2,'Ctrvar1':Ctrvar1,'Ctrvar2':Ctrvar2,'Cosvar1':Cosvar1,'Cosvar2':Cosvar2}))

#cercle des corrélations
fig, axes = plt.subplots(figsize=(8,8))
axes.set_xlim(-1,1)
axes.set_ylim(-1,1)
#affichage des étiquettes (noms des variables)
for j in range(p):
 plt.annotate(df.columns[j],(corvar1[j],corvar2[j]))
#ajouter les axes
plt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1)
plt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)
#ajouter un cercle
cercle = plt.Circle((0,0),1,color='blue',fill=False)
axes.add_artist(cercle)
#affichage
plt.show()

"""# **Kmeans**"""

kmeans = cluster.KMeans(n_clusters=3)
kmeans.fit(df)
idk = np.argsort(kmeans.labels_)
print(pd.DataFrame(df.index[idk],kmeans.labels_[idk]))

A=kmeans.transform(df)
print(A)

acp= PCA(n_components =2).fit_transform(df)
for couleur,k in zip([ 'red','blue','lawngreen'],[0,1,2]) :
  plt.scatter(acp[kmeans.labels_==k,0], acp[kmeans.labels_==k,1],c=couleur)
n = df.shape[0]
for i in range(n) :
  plt.annotate(df.index[i],(acp[i,0], acp[i,1]))
plt.title('Projection of individuals on plane PC1-PC2')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

df['Classe']=kmeans.labels_[idk]
#statistiques selon la classe
C = df.groupby('Classe')
pd.set_option('display.max_columns', None)
print(C.describe(include='all'))

corr=df.corr()
print(corr)

eig_vals, eig_vecs = np.linalg.eig(corr)
print(eig_vals)

#instanciation
sc = StandardScaler()
#transformation – centrage-réduction
Z = sc.fit_transform(df)
#classe pour l'ACP
from sklearn.decomposition import PCA
#instanciation
acp = PCA()
print(acp)
#calculs
coord = acp.fit_transform(Z)        #centrage&réduction
print(coord)

#contribution des individus dans l'inertie totale
A=coord**2                                     #mult par element
Ctr1=(A[:,0]/(n*eig_vals[0]))*100            #contribution 1er axe
B=Z**2
normind=np.sum(Z**2,axis=1)
Cos1=(A[:,0]/normind)*100
#contribution des individus dans l'inertie totale
A=coord**2
Ctr2=(A[:,1]/(n*eig_vals[1]))*100                     #contribution   2eme axe
Cos2=(A[:,1]/normind)*100
print(pd.DataFrame({'ID':df.index,'Contr_1':Ctr1,'Contr_2':Ctr2,'Coscar_1':Cos1,'Coscar_2':Cos2, }))

sqrt_eigval = np.sqrt(eig_vals)
corvar1=acp.components_[0,:] * sqrt_eigval[0]         #Uj:acp.components_
print(corvar1)
corvar2=acp.components_[1,:] * sqrt_eigval[1]
print(corvar2)

#contribution des variables 
Ctrvar1=(corvar1**2)/(eig_vals[0])*100
Ctrvar2=(corvar2**2)/(eig_vals[1])*100
#qualité des variables 
Cosvar1=(corvar1**2)*100
Cosvar2=(corvar2**2)*100
print(pd.DataFrame({'ID':df.columns,'corvar1':corvar1,'corvar2':corvar2,'Ctrvar1':Ctrvar1,'Ctrvar2':Ctrvar2,'Cosvar1':Cosvar1,'Cosvar2':Cosvar2, }))

#cercle des corrélations
fig, axes = plt.subplots(figsize=(8,8))
axes.set_xlim(-1,1)
axes.set_ylim(-1,1)
#affichage des étiquettes (noms des variables)
for j in range(p):
 plt.annotate(df.columns[j],(corvar1[j],corvar2[j]))
#ajouter les axes
plt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1)
plt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)
#ajouter un cercle
cercle = plt.Circle((0,0),1,color='blue',fill=False)
axes.add_artist(cercle)
#affichage
plt.show()

"""# **Classification Ascendante Hiérarchique (CAH)**"""

# normalisation des données
sc = StandardScaler()
df_norm = sc.fit_transform(df)

#générer la matrice des liens
Z = linkage(df,method='ward',metric='euclidean')
print(Z)

# affichage du dendrogramme
plt.figure(figsize=(10, 8))
dendrogram(Z, labels=df.index, color_threshold=300)
plt.title("Dendrogramme de la CAH")
plt.xlabel("Observations")
plt.ylabel("Distance")
plt.show()

#Détermination des groupes à partir du dendrogramme
groupes_cah= fcluster(Z,t =300,criterion ='distance')
print (groupes_cah)

#index triés des groupes
idg = np.argsort (groupes_cah)
#affichage des observations et leurs groupes
df_groupes = pd.DataFrame(df.index[idg], groupes_cah[idg], columns=['Observations'])
print(df_groupes)